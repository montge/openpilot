{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Analysis with the Test Harness\n",
    "\n",
    "This notebook demonstrates how to use the algorithm test harness to:\n",
    "1. Load and visualize test scenarios\n",
    "2. Run algorithms against scenarios\n",
    "3. Analyze and compare algorithm performance\n",
    "4. Generate reports and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Algorithm harness imports\n",
    "from openpilot.selfdrive.controls.lib.tests.algorithm_harness import (\n",
    "    ScenarioRunner,\n",
    "    generate_synthetic_scenario,\n",
    ")\n",
    "from openpilot.selfdrive.controls.lib.tests.algorithm_harness.adapters import (\n",
    "    LatControlPIDAdapter,\n",
    "    LatControlTorqueAdapter,\n",
    ")\n",
    "from openpilot.selfdrive.controls.lib.tests.algorithm_harness.metrics import (\n",
    "    format_metrics_table,\n",
    "    compare_metrics,\n",
    ")\n",
    "\n",
    "# Configure matplotlib for notebook display\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Test Scenarios\n",
    "\n",
    "Generate synthetic scenarios for testing. The harness supports various scenario types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of test scenarios\n",
    "scenarios = [\n",
    "    generate_synthetic_scenario(\n",
    "        name=\"constant_target\",\n",
    "        duration_s=5.0,\n",
    "        scenario_type=\"constant\",\n",
    "        target=0.0,\n",
    "    ),\n",
    "    generate_synthetic_scenario(\n",
    "        name=\"sine_wave\",\n",
    "        duration_s=10.0,\n",
    "        scenario_type=\"sine\",\n",
    "        amplitude=0.01,\n",
    "        frequency=0.2,\n",
    "    ),\n",
    "    generate_synthetic_scenario(\n",
    "        name=\"step_response\",\n",
    "        duration_s=5.0,\n",
    "        scenario_type=\"step\",\n",
    "        step_time=2.0,\n",
    "        step_value=0.005,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(scenarios)} scenarios:\")\n",
    "for s in scenarios:\n",
    "    print(f\"  - {s.name}: {len(s)} steps, {s.metadata.get('duration_s', 0):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Scenario Ground Truth\n",
    "\n",
    "Plot the ground truth (target) values to understand what each scenario tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, scenario in zip(axes, scenarios):\n",
    "    if scenario.ground_truth:\n",
    "        t = np.arange(len(scenario.ground_truth)) * 0.01  # assuming 100Hz\n",
    "        ax.plot(t, scenario.ground_truth, 'b-', linewidth=2)\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Target Value')\n",
    "        ax.set_title(scenario.name)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Algorithms Against Scenarios\n",
    "\n",
    "Execute algorithms and collect performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize runner and algorithms\n",
    "runner = ScenarioRunner(deterministic=True)\n",
    "pid_adapter = LatControlPIDAdapter()\n",
    "torque_adapter = LatControlTorqueAdapter()\n",
    "\n",
    "# Run PID algorithm\n",
    "pid_results = []\n",
    "for scenario in scenarios:\n",
    "    result = runner.run(pid_adapter, scenario, \"LatControlPID\")\n",
    "    pid_results.append(result)\n",
    "    print(f\"PID on {scenario.name}: RMSE={result.metrics.tracking_error_rmse:.6f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Run Torque algorithm\n",
    "torque_results = []\n",
    "for scenario in scenarios:\n",
    "    result = runner.run(torque_adapter, scenario, \"LatControlTorque\")\n",
    "    torque_results.append(result)\n",
    "    print(f\"Torque on {scenario.name}: RMSE={result.metrics.tracking_error_rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Algorithm Outputs\n",
    "\n",
    "Visualize how each algorithm tracks the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(scenarios), 1, figsize=(12, 4*len(scenarios)))\n",
    "\n",
    "for ax, scenario, pid_result, torque_result in zip(axes, scenarios, pid_results, torque_results):\n",
    "    t = np.arange(len(scenario)) * 0.01\n",
    "    \n",
    "    # Plot ground truth\n",
    "    if scenario.ground_truth:\n",
    "        ax.plot(t, scenario.ground_truth, 'k--', label='Ground Truth', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Plot algorithm outputs\n",
    "    ax.plot(t[:len(pid_result.outputs)], pid_result.outputs, 'b-', label='PID', linewidth=1.5)\n",
    "    ax.plot(t[:len(torque_result.outputs)], torque_result.outputs, 'r-', label='Torque', linewidth=1.5)\n",
    "    \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Output')\n",
    "    ax.set_title(f'{scenario.name} - Algorithm Comparison')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Metrics\n",
    "\n",
    "Compare performance metrics between algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for comparison\n",
    "metrics_data = {\n",
    "    'Scenario': [],\n",
    "    'PID RMSE': [],\n",
    "    'Torque RMSE': [],\n",
    "    'PID Smoothness': [],\n",
    "    'Torque Smoothness': [],\n",
    "    'PID Latency (ms)': [],\n",
    "    'Torque Latency (ms)': [],\n",
    "}\n",
    "\n",
    "for scenario, pid_r, torque_r in zip(scenarios, pid_results, torque_results):\n",
    "    metrics_data['Scenario'].append(scenario.name)\n",
    "    metrics_data['PID RMSE'].append(pid_r.metrics.tracking_error_rmse)\n",
    "    metrics_data['Torque RMSE'].append(torque_r.metrics.tracking_error_rmse)\n",
    "    metrics_data['PID Smoothness'].append(pid_r.metrics.output_smoothness)\n",
    "    metrics_data['Torque Smoothness'].append(torque_r.metrics.output_smoothness)\n",
    "    metrics_data['PID Latency (ms)'].append(pid_r.metrics.latency_mean_ms)\n",
    "    metrics_data['Torque Latency (ms)'].append(torque_r.metrics.latency_mean_ms)\n",
    "\n",
    "# Display as table\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(metrics_data)\n",
    "df.set_index('Scenario', inplace=True)\n",
    "display(df.style.format('{:.6f}').background_gradient(cmap='RdYlGn_r', subset=['PID RMSE', 'Torque RMSE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metrics Visualization\n",
    "\n",
    "Create bar charts comparing key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "x = np.arange(len(scenarios))\n",
    "width = 0.35\n",
    "scenario_names = [s.name for s in scenarios]\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0].bar(x - width/2, metrics_data['PID RMSE'], width, label='PID', color='#007acc')\n",
    "axes[0].bar(x + width/2, metrics_data['Torque RMSE'], width, label='Torque', color='#dc3545')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('Tracking Error (RMSE)')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(scenario_names, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "\n",
    "# Smoothness comparison\n",
    "axes[1].bar(x - width/2, metrics_data['PID Smoothness'], width, label='PID', color='#007acc')\n",
    "axes[1].bar(x + width/2, metrics_data['Torque Smoothness'], width, label='Torque', color='#dc3545')\n",
    "axes[1].set_ylabel('Smoothness (jerk RMS)')\n",
    "axes[1].set_title('Output Smoothness')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(scenario_names, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "\n",
    "# Latency comparison\n",
    "axes[2].bar(x - width/2, metrics_data['PID Latency (ms)'], width, label='PID', color='#007acc')\n",
    "axes[2].bar(x + width/2, metrics_data['Torque Latency (ms)'], width, label='Torque', color='#dc3545')\n",
    "axes[2].set_ylabel('Latency (ms)')\n",
    "axes[2].set_title('Mean Update Latency')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(scenario_names, rotation=45, ha='right')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Metrics Report\n",
    "\n",
    "Generate a formatted metrics report for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed metrics for the sine wave scenario\n",
    "sine_idx = 1  # sine_wave scenario index\n",
    "\n",
    "print(format_metrics_table(pid_results[sine_idx].metrics, \"LatControlPID\"))\n",
    "print()\n",
    "print(format_metrics_table(torque_results[sine_idx].metrics, \"LatControlTorque\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using the Comparison API\n",
    "\n",
    "Use the built-in comparison functionality for structured analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the runner's compare method\n",
    "comparison = runner.compare(\n",
    "    baseline=pid_adapter,\n",
    "    candidate=torque_adapter,\n",
    "    scenarios=scenarios,\n",
    "    baseline_name=\"PID\",\n",
    "    candidate_name=\"Torque\",\n",
    ")\n",
    "\n",
    "# Display aggregate comparison\n",
    "print(\"=\" * 60)\n",
    "print(\"AGGREGATE COMPARISON: PID vs Torque\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "agg = comparison['aggregate']['comparison']\n",
    "for metric, data in agg.items():\n",
    "    if metric in ['tracking_error_rmse', 'output_smoothness', 'latency_mean_ms']:\n",
    "        symbol = \"better\" if data['improved'] else \"worse\"\n",
    "        print(f\"{metric}:\")\n",
    "        print(f\"  PID:    {data['baseline']:.6f}\")\n",
    "        print(f\"  Torque: {data['candidate']:.6f} ({data['pct_change']:+.1f}% - {symbol})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Implementing Custom Algorithms\n",
    "\n",
    "Example of creating and testing a custom algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpilot.selfdrive.controls.lib.tests.algorithm_harness.interface import (\n",
    "    AlgorithmState,\n",
    "    AlgorithmOutput,\n",
    ")\n",
    "\n",
    "class SimpleProportionalController:\n",
    "    \"\"\"A simple proportional controller for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, gain: float = 1.0):\n",
    "        self.gain = gain\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        pass  # No state to reset\n",
    "    \n",
    "    def update(self, state: AlgorithmState) -> AlgorithmOutput:\n",
    "        # Simple proportional control\n",
    "        output = self.gain * 0.1  # Simplified - in practice, use state values\n",
    "        saturated = abs(output) >= 1.0\n",
    "        output = max(-1.0, min(1.0, output))\n",
    "        \n",
    "        return AlgorithmOutput(\n",
    "            output=output,\n",
    "            saturated=saturated,\n",
    "        )\n",
    "\n",
    "# Test custom algorithm\n",
    "custom_algo = SimpleProportionalController(gain=0.5)\n",
    "result = runner.run(custom_algo, scenarios[0], \"CustomP\")\n",
    "\n",
    "print(f\"Custom algorithm results:\")\n",
    "print(f\"  RMSE: {result.metrics.tracking_error_rmse:.6f}\")\n",
    "print(f\"  Smoothness: {result.metrics.output_smoothness:.6f}\")\n",
    "print(f\"  Latency: {result.metrics.latency_mean_ms:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Scenario Generation**: Creating synthetic test scenarios with various profiles\n",
    "2. **Algorithm Execution**: Running existing and custom algorithms\n",
    "3. **Metrics Collection**: Automatic collection of tracking, smoothness, and latency metrics\n",
    "4. **Comparison Analysis**: Comparing algorithms across multiple scenarios\n",
    "5. **Visualization**: Creating plots to understand algorithm behavior\n",
    "\n",
    "For more details, see the [Algorithm Harness README](../README.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
