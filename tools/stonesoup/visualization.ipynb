{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stone Soup Tracking Algorithm Comparison\n",
    "\n",
    "Visualize and compare openpilot's KF1D filter against Stone Soup's Kalman variants,\n",
    "multi-target trackers, and fusion methods.\n",
    "\n",
    "**Prerequisites**: `pip install stonesoup matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Filter Comparison: KF1D vs Stone Soup Kalman Variants\n",
    "\n",
    "Compare openpilot's production KF1D against Kalman, Extended Kalman,\n",
    "Unscented Kalman, Cubature Kalman, and Particle filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpilot.tools.stonesoup.comparison import (\n",
    "    create_constant_velocity_scenario,\n",
    "    compare_filters,\n",
    "    format_comparison_report,\n",
    ")\n",
    "\n",
    "scenario = create_constant_velocity_scenario(dt=0.05, duration=10.0)\n",
    "metrics = compare_filters(scenario)\n",
    "print(format_comparison_report(scenario, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot position estimates vs ground truth\n",
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "times = [r.timestamp for r in scenario.results['KF1D']]\n",
    "gt_pos = [r.ground_truth[0] for r in scenario.results['KF1D']]\n",
    "measurements = [r.measurement for r in scenario.results['KF1D'] if r.measurement is not None]\n",
    "meas_times = [r.timestamp for r in scenario.results['KF1D'] if r.measurement is not None]\n",
    "\n",
    "axes[0].plot(times, gt_pos, 'k--', label='Ground Truth', linewidth=2)\n",
    "axes[0].scatter(meas_times, measurements, c='gray', s=10, alpha=0.5, label='Measurements')\n",
    "\n",
    "for name, results in scenario.results.items():\n",
    "    pos = [r.state[0] for r in results]\n",
    "    axes[0].plot(times, pos, label=name, alpha=0.8)\n",
    "\n",
    "axes[0].set_ylabel('Position (m)')\n",
    "axes[0].set_title('Filter Position Estimates')\n",
    "axes[0].legend(fontsize=8, ncol=3)\n",
    "\n",
    "# Plot position error\n",
    "for name, results in scenario.results.items():\n",
    "    errors = [abs(r.state[0] - r.ground_truth[0]) for r in results]\n",
    "    axes[1].plot(times, errors, label=name, alpha=0.8)\n",
    "\n",
    "axes[1].set_ylabel('Position Error (m)')\n",
    "axes[1].set_xlabel('Time (s)')\n",
    "axes[1].set_title('Absolute Position Error')\n",
    "axes[1].legend(fontsize=8, ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE comparison bar chart\n",
    "names = [m.filter_name for m in metrics]\n",
    "rmse_pos = [m.rmse_position for m in metrics]\n",
    "rmse_vel = [m.rmse_velocity for m in metrics]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(names)))\n",
    "axes[0].bar(names, rmse_pos, color=colors)\n",
    "axes[0].set_ylabel('RMSE (m)')\n",
    "axes[0].set_title('Position RMSE by Filter')\n",
    "axes[0].tick_params(axis='x', rotation=30)\n",
    "\n",
    "axes[1].bar(names, rmse_vel, color=colors)\n",
    "axes[1].set_ylabel('RMSE (m/s)')\n",
    "axes[1].set_title('Velocity RMSE by Filter')\n",
    "axes[1].tick_params(axis='x', rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Target Tracking\n",
    "\n",
    "Compare JPDA and GNN trackers on highway scenarios with MOTA/MOTP metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpilot.tools.stonesoup.multi_target import (\n",
    "    create_highway_scenario,\n",
    "    compare_multi_target_trackers,\n",
    "    format_tracking_report,\n",
    ")\n",
    "\n",
    "mt_scenario = create_highway_scenario(dt=0.1, duration=10.0, n_vehicles=4)\n",
    "mt_metrics = compare_multi_target_trackers(mt_scenario)\n",
    "print(format_tracking_report(mt_scenario, mt_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-target tracking metrics comparison\n",
    "tracker_names = list(mt_metrics.keys())\n",
    "mota_vals = [mt_metrics[n].mota for n in tracker_names]\n",
    "motp_vals = [mt_metrics[n].motp for n in tracker_names]\n",
    "id_switches = [mt_metrics[n].id_switches for n in tracker_names]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(tracker_names, mota_vals, color=['steelblue', 'coral'])\n",
    "axes[0].set_ylabel('MOTA')\n",
    "axes[0].set_title('Multi-Object Tracking Accuracy')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "axes[1].bar(tracker_names, motp_vals, color=['steelblue', 'coral'])\n",
    "axes[1].set_ylabel('MOTP (m)')\n",
    "axes[1].set_title('Multi-Object Tracking Precision')\n",
    "\n",
    "axes[2].bar(tracker_names, id_switches, color=['steelblue', 'coral'])\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('ID Switches')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Track Fusion\n",
    "\n",
    "Compare Covariance Intersection fusion against single-sensor baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpilot.tools.stonesoup.track_fusion import (\n",
    "    create_fusion_scenario,\n",
    "    compare_fusion_methods,\n",
    "    format_fusion_report,\n",
    ")\n",
    "\n",
    "fusion_scenario = create_fusion_scenario(dt=0.05, duration=5.0)\n",
    "fusion_metrics = compare_fusion_methods(fusion_scenario)\n",
    "print(format_fusion_report(fusion_scenario, fusion_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion RMSE comparison\n",
    "fusion_names = list(fusion_metrics.keys())\n",
    "fusion_rmse = [fusion_metrics[n].rmse_position for n in fusion_names]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "colors = ['steelblue', 'coral', 'mediumseagreen', 'goldenrod'][:len(fusion_names)]\n",
    "ax.bar(fusion_names, fusion_rmse, color=colors)\n",
    "ax.set_ylabel('Position RMSE (m)')\n",
    "ax.set_title('Fusion Method Comparison')\n",
    "ax.tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark Scenarios\n",
    "\n",
    "Run all benchmark scenarios and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpilot.tools.stonesoup.benchmarks.scenarios import (\n",
    "    create_highway_scenario as bm_highway,\n",
    "    create_cut_in_scenario,\n",
    "    create_cut_out_scenario,\n",
    "    create_multi_vehicle_scenario,\n",
    "    create_occlusion_scenario,\n",
    "    create_noisy_scenario,\n",
    ")\n",
    "\n",
    "scenarios = {\n",
    "    'Highway': bm_highway(),\n",
    "    'Cut-in': create_cut_in_scenario(),\n",
    "    'Cut-out': create_cut_out_scenario(),\n",
    "    'Multi-vehicle': create_multi_vehicle_scenario(),\n",
    "    'Occlusion': create_occlusion_scenario(),\n",
    "    'Noisy (weather)': create_noisy_scenario(),\n",
    "}\n",
    "\n",
    "print(f'Loaded {len(scenarios)} benchmark scenarios:')\n",
    "for name, s in scenarios.items():\n",
    "    n_steps = len(s.ground_truth) if hasattr(s, 'ground_truth') else 'N/A'\n",
    "    print(f'  {name}: {n_steps} timesteps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scenario trajectories\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "for ax, (name, s) in zip(axes.flat, scenarios.items()):\n",
    "    if hasattr(s, 'ground_truth'):\n",
    "        gt = s.ground_truth\n",
    "        if hasattr(gt[0], 'x'):\n",
    "            x = [p.x for p in gt]\n",
    "            y = [p.y for p in gt]\n",
    "            ax.plot(x, y, 'k-', linewidth=2)\n",
    "            ax.set_xlabel('x (m)')\n",
    "            ax.set_ylabel('y (m)')\n",
    "        else:\n",
    "            times = range(len(gt))\n",
    "            ax.plot(times, gt, 'k-', linewidth=2)\n",
    "            ax.set_xlabel('Timestep')\n",
    "            ax.set_ylabel('Value')\n",
    "    ax.set_title(name)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Benchmark Scenario Ground Truth Trajectories', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
